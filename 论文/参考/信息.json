[
    {
        "title": "Edge computing collaborative offloading strategy for space‐air‐ground integrated networks",
        "abstract": "Due to geographical factors, it is impossible to build large‐scale communication network infrastructure in remote areas, which leads to poor network communication quality in these areas and a series of delay‐sensitive tasks cannot be timely processed and responded. Aiming at the problem of limited coverage in remote areas, the space‐air‐ground integrated networks (SAGIN) combined with mobile edge computing (MEC) can provide low latency and high reliability transmission for offloading delay‐sensitive tasks for users in remote areas. Considering the strong limitation of satellite resources in the space‐ground integrated network and insufficient energy of local user equipment, firstly, a satellite‐UAV cluster‐ground three‐layer edge computing network architecture is proposed in this paper. Under the condition that the delay requirements of various ground tasks are met, the task offloading problem is transformed into a Stackelberg game between ground user equipment and edge servers. In addition, it is proved that the existence of Nash equilibrium in non‐cooperative game between ground user equipment by using potential game. Finally, a Nash equilibrium iterative offloading algorithm based on Stackelberg game (NEIO‐SG) is proposed to find the optimal offloading strategy for tasks to minimize the system offloading cost and the optimal forwarding percentage strategy for offloading tasks to maximize the utility function of the edge server. Simulation results show that compared to other baseline algorithms, NEIO‐SG reduces the total system latency during task offloading by about 13%$$ \\% $$ and the energy consumption of the edge server by about 35%$$ \\% $$.\nWiley Online Library",
        "bibid": "xiang2024edgex7b"
    },
    {
        "title": "Application of improved ant colony algorithm in load balancing of software-defined networks",
        "abstract": "Software-defined networking (SDN) separates the forwarding plane and control plane of the network equipment, adopts a centralized control mode to simplify network deployment and improve network management efficiency and realizes the network flexible control and management of traffic through programmable open interfaces. At present, it has been widely used in domestic and international data centre networks. With the explosive growth of the scale of data centres and the increase in user requirements for service quality, load balancing and congestion control of data centres have become significant issues in current research. After studying and analysing data centre SDN architecture and load balancing problems in detail, certain experts proposed an SDN load balancing algorithm, based on improved ant colony optimization-load balancing (IACO-LB). Firstly, the overall framework of SDN load balancing in data centres is studied, which is mainly divided into three parts: basic network equipment, OpenFlow protocol and controller. Among them, the controller constitutes the core of the entire load balancing system, including four modules: network topology awareness, status collection, the core of the load balancing algorithm and the flow table distribution. Then, an SDN load balancing algorithm, based on the improved ant colony optimization (IACO) is proposed to achieve dynamic load balancing of the SDN. The algorithm fully considers the performance parameters of network links and servers, and its design is based on the principle of selecting links and servers with low utilization. The evaluation methods of server module and link module are designed and the ant colony algorithm is used to find the global optimal solution. In order to prevent the algorithm from falling into local optimum, the Kent chaotic model is adopted to disturb the transition probability of the ant colony, by improving the basic ant colony algorithm. Finally, a network topology model was established in MATLAB to carry out simulation experiments. The results show that, compared with the equivalent multi-path algorithm and path server traffic scheduling algorithm, IACO-LB can effectively solve the load balancing problem of SDN and can dynamically adjust the routing scheme, according to the changes in network link traffic and server utilization. The algorithm converges quickly and can achieve a better global load balancing scheme.\nSpringer",
        "bibid": "zheng2023application362"
    },
    {
        "title": "A Novel Scheduling Scheme for Earth Observation in LEO Satellite Systems",
        "abstract": "Earth observation applications, such as emergency surveillance and disaster relief, are thriving due to the availability of earth observation satellites that provide timely and objective observation data at different spatial and temporal scales. Such observation data is processed at the satellite edge with orbital edge computing, leading to potentially reduced bandwidth cost and transmission delay. However, most existing studies primarily focus on optimizing computation offloading but ignore the consideration of object observation and observation data transmission. To fill this gap, this paper proposes a novel scheduling approach that jointly considers observation satellites, relay satellites, and computing satellites in LEO satellite systems, aiming to maximize the number of completed observation tasks while taking into account various requirements of observation, transmission, and computation resources. Specifically, we first formulate the problem of jointly scheduling observation, relay, and computing satellites to maximize the number of accomplished observation tasks. Then, we decompose the formulated problem into two sub-problems and design a resource-aware algorithm called ORCA to determine the optimal scheduling of observation, relay, and computing satellites. Simulation results demonstrate that ORCA outperforms existing algorithms in terms of completing a number of observation tasks.\nieeexplore.ieee.org",
        "bibid": "N/A"
    },
    {
        "title": "Efficient bandwidth allocation in SDN-based peer-to-peer data streaming using machine learning algorithm",
        "abstract": "Software-defined networking (SDN) is a contemporary structural design paradigm that aspires to correct bandwidth-efficient usage and user application transparent interoperability. It claims to be self-motivated, convenient, affordable, and programmable. The network control may become directly programmable thanks to SDN architecture's decoupling of the network control and management plane from the data plane. Due to the centralized architecture of the SDN network, message propagation to various network devices in SDN is delayed. The SDN controller must thus establish new rules for each new communication. Peer-to-peer protocols are deployed in the system for the fast propagation of contents over the layered architecture. However, these protocols produce unwanted packets because these protocols mostly use a reactive routing protocol over an SDN architecture. This causes the delay for different applications due to inefficient bandwidth management. By offering resistance to connection failures and accommodating constantly changing bandwidth needs, it is difficult to control the needed bandwidth. This paper proposes a new technique using the Random Forest algorithm for efficient bandwidth management for peer-to-peer applications. In this technique, bandwidth usage of different applications is computed and predicted. So, according to the need of applications, network traffic is adjusted accordingly. An ultra-peer node is responsible for communication with other nodes in this methodology, eliminating unwanted traffic across the network. A Linux-based OpenvSwitch, POX controller and Mininet-based SDN-based network system are used for the tests. The results of the experiment demonstrate that the proposed framework may significantly improve QoS while outperforming the current bandwidth allocation algorithms in terms of success rate, throughput, response time, etc.\nSpringer",
        "bibid": "aldabbas2023efficientqba"
    },
    {
        "title": "Research on Edge Server Deployment Strategy in LEO Mega-Constellation",
        "abstract": "The integration of Mobile Edge Computing (MEC) and Low Earth Orbit (LEO) satellite networks holds the potential to offer ubiquitous computing services for ground users and has garnered significant attention recently. While there has been extensive research conducted in the field of Satellite Mobile Edge Computing (SMEC), research on edge server deployment in mega-constellation is overlooked. Edge servers require an appropriate quantification and placement before the implementation of computation offloading. The improper deployment strategy can result in high access latency and imbalance workload. In this paper, we propose a two-stage approach, called cluster-based small-scale server deloyment (CSSD), for small-scale placing and dynamic allocating edge servers that enable low access latency and workload balancing. Specifically, the offline stage is employed to determine the optimal placement of edge servers and the initial offloading mapping from access satellites to service satellites. The online stage, building dynamically adjusts the offloading mapping based on the spatiotemporal positions and workload of the satellites to balance system workload. Evaluation results show that CSSD outperforms other approaches with up to 16.37% and 35.38% enhancements in terms of workload standard deviation and user service rate while maintaining low access latency.\nieeexplore.ieee.org",
        "bibid": "ding2024researchxlp"
    },
    {
        "title": "Performance analysis of federated learning in orbital edge computing",
        "abstract": "Federated Learning (FL) is a promising solution for collaborative machine learning while respecting data privacy and locality. FL has been used in Low Earth Orbit (LEO) satellite constellations for different space applications including earth observation, navigation, and positioning. Orbital Edge Computing (OEC) refers to the deployment of edge computing resources and data processing capabilities in space-based systems, enabling real-time data analysis and decision-making for remote and space-based applications. While there is existing research exploring the integration of federated learning in OEC, the influence of diverse factors such as space conditions, communication constraints, and machine learning models remains uncertain. This paper addresses this gap and presents a comprehensive performance analysis of FL methods in the unique and challenging setting of OEC. We consider model accuracy, training time, and power consumption as the performance metrics under different working conditions including IID and non-IID data distributions to analyse the performance of centralised and decentralised FL approaches. The experimental results demonstrate that although the asynchronous centralised FL method has high fluctuations in the accuracy curve, it is suitable for space applications in which power consumption and training time are two main factors. In addition, the number of sampled satellites for decentralised FL methods is an important parameter in non-IID data distribution. Moreover, increasing altitude can reduce the training time and increase the power consumption. This study enables us to highlight a number of performance challenges in OEC for further investigation.\nACM Digital Library",
        "bibid": "jabbarpour2023performancevvr"
    },
    {
        "title": "Satellite Computing: From Space to Your Screen",
        "abstract": "The space industry is undergoing a transformative shift driven by the rapid growth of LEO satellite mega-constellations. These constellations cater to growing demands in various sectors, from intelligent transportation and smart cities to maritime surveillance and disaster response. Satellite computing emerges as a pivotal foundation in this evolution. In our tutorial lecture, we embark on a journey into the realm of satellite computing, a burgeoning field with immense potential. We begin by addressing a fundamental question: What is satellite computing? We delve into core concepts, revealing how satellites can function as computational powerhouses orbiting our planet. As we progress, we explore diverse scenarios where satellite computing shines. We also confront the unique challenges it faces in space’s harsh environment, featuring deep vacuum conditions, radiation exposure, strong vibrations, and extreme temperature ranges. Our tutorial offers insights into our research in satellite computing. We share practical experiences from deploying the Tiansuan constellation, showcasing the real-world applications of these cutting-edge technologies. Our vision is to democratize satellite computing access. By transforming satellites into servers “with wings”, we envision a future where every corner of the globe reaps the benefits of satellite computing’s vast potential.\nSpringer",
        "bibid": "N/A"
    },
    {
        "title": "Energy-efficient Federated Learning for Earth Observation in LEO Satellite Systems",
        "abstract": "Low Earth Orbit (LEO) satellites enable various Internet of Things (IoT) applications by providing plentiful observation data at different spatial scales. However, transmitting such a large volume of observation data to the ground for model training is challenging due to the intermittent connection between satellites and ground stations, leading to long training delays for task-oriented artificial intelligence models. Orbital federated learning has been investigated to enable on-board model training sharing model parameters instead of observation data, thus greatly reducing the communication overhead. However, these works underutilize the availability of inter-satellite links and ignore energy consumption. To fill this gap, we propose an energy-efficient federated learning scheme for earth observation applications in LEO satellite systems to minimize learning loss and energy consumption. Specifically, to reduce the effect of data heterogeneity on learning accuracy, we propose a satellite grouping scheme based on satellites' data distribution and communication delay to ground stations. Then, we optimize each satellite's transmission power and computing frequency under the constraint of training time. Experimental results based on popular datasets show the efficacy of the proposed scheme compared to benchmark methods.\nieeexplore.ieee.org",
        "bibid": "N/A"
    },
    {
        "title": "Supporting the Natural Disaster Management Distributing Federated Intelligence over the Cloud-Edge Continuum: the TEMA Architecture",
        "abstract": "Natural disasters are more and more often present in our daily life. Many are the cases where these events affect people and economies. In this context, there is the need for a technological intervention in support of first responders, with solutions capable of make decisions on the disaster areas. Indeed, considering these scenarios are time-sensitive, the intention is moving the computation units closer to those areas. In this paper, we propose a computing continuum architecture for offloading distributed intelligences over cloud, edge and deep edge layers. Exploiting the federated learning paradigm, enables mobile and stationary devices to independently train local models, contributing to the creation of the global common model.\nACM Digital Library",
        "bibid": "carnevale2023supportingj02"
    },
    {
        "title": "Reinforcement Learning Based Intelligent Routing for Software Defined LEO Satellite Networks",
        "abstract": "The low earth orbit (LEO) satellite constellation is regarded as an effective complement to the terrestrial communication system due to its seamless coverage and ultra-low latency. Unfortunately, the highly dynamic traffic volume, as well as the inherent nature of dynamic topology changes caused by frequent link handover and uncertain hardware failures, pose severe challenges in the design of reliable routing. However, most existing reliable routing approaches with distributed schemes only focus on information exchange between adjacent nodes, which makes them fail to perceive real-time global network changes and make optimal decisions. In this paper, we propose a software defined networking (SDN) based intelligent satellite routing (SISR) method to increase the adaptivity and reliability during the packet transmission process. With the facilitation of SDN, we manage the network in a hierarchical and centralized paradigm, and further implement a more refined form of reinforcement learning (RL) to enhance the fault-tolerant ability of satellite network routing. Experimental results show that our solution can reduce latency and packet loss ratio by more than 42% and 29% compared to baselines.\nieeexplore.ieee.org",
        "bibid": "fu2023reinforcementncv"
    },
    {
        "title": "Content-Aware Proportional Caching for Efficient Data Delivery over Satellite Network",
        "abstract": "The Low Earth Orbit (LEO) satellite network has emerged as a crucial infrastructure for global content delivery service, due to its global coverage and low latency. However, the high mobility of satellites and the limited on-board resources impede its data distribution efficiency. In this paper, we propose CA-prop, a content-aware proportional caching scheme to improve the efficiency of content delivery over limited on-board cache resources. CA-prop captures the time-varying requesting feature of contents and leverage this metric to determine the caching proportion of a specific type of content. Based on the satellite weighted time-varying graph, a cooperative cache placement scheme is developed to cache the proportional content along the data delivery path. Furthermore, with prediction of satellite mobility and data delivery direction, we enhance CA-prop with a directional pre-caching strategy to increase the caching service residence time on the mobile large-scale satellite constellation. Our experiment results demonstrate that our strategy is effective in enhancing content distribution efficiency, with an average reduction of 43% in data delivery delay and 90% improvement in cache hit ratio under various content scenarios as compared to state-of-the-art approaches.\nieeexplore.ieee.org",
        "bibid": "zhang2023contentq6k"
    },
    {
        "title": "A modeling-based approach for dependability analysis of a constellation of satellites",
        "abstract": "Satellite constellations play critical roles across various sectors, encompassing communication, Earth observation and space exploration. Ensuring the dependable operation of these constellations is of utmost importance. This paper introduces a dependability modeling approach using stochastic Petri nets to analyze satellite constellations. The primary focus is on improving operational efficiency through the assessment of availability, reliability and maintainability. The approach helps satellite designers make informed decisions when selecting constellation configurations by assessing various dependability metrics. Using a global navigation satellite system as a case study, we conduct extensive numerical experiments to evaluate the feasibility of our approach. The results demonstrate quantitatively the significant impact of redundant components on both reliability and availability. They also illustrate how utilizing satellites in repair and operational orbits can influence these metrics and highlight the direct correlation between reliability and maintainability.\nSpringer",
        "bibid": "farias2024modelingvxj"
    },
    {
        "title": "Artificial Intelligence in 6G Wireless Networks: Opportunities, Applications, and Challenges",
        "abstract": "Wireless technologies are growing unprecedentedly with the advent and increasing popularity of wireless services worldwide. With the advancement in technology, profound techniques can potentially improve the performance of wireless networks. Besides, the advancement of artificial intelligence (AI) enables systems to make intelligent decisions, automation, data analysis, insights, predictive capabilities, learning, and adaptation. A sophisticated AI will be required for next‐generation wireless networks to automate information delivery between smart applications simultaneously. AI technologies, such as machines and deep learning techniques, have attained tremendous success in many applications in recent years. Hances, researchers in academia and industry have turned their attention to the advanced development of AI‐enabled wireless networks. This paper comprehensively surveys AI technologies for different wireless networks with various applications. Moreover, we present various AI‐enabled applications that exploit the power of AI to enable the desired evolution of wireless networks. Besides, the challenges of unsolved research in this area, which represent the future research trends of AI‐enabled wireless networks, are discussed in detail. We provide several suggestions and solutions that help wireless networks be more intelligent and sophisticated to handle complicated problems. In summary, this paper can help researchers deeply understand the up‐to‐the‐minute wireless network designs based on AI technologies and identify interesting unsolved issues to be pursued in their research in a fast way.\nWiley Online Library",
        "bibid": "alhammadi2024artificialpgw"
    },
    {
        "title": "[HTML] Proposal and investigation of a distributed learning strategy in Orbital Edge Computing-endowed satellite networks for Earth Observation applications",
        "abstract": "One of the key enabling solutions to in-orbit extract information from Earth Observation images is given by deep learning techniques. However, the accuracy of these algorithms is strictly related to the availability of large datasets of satellite images for training purposes. Limitations on the available transmission bandwidth in the orbital context may prevent the possibility to downlink all acquired images to a node where centralized training happens. Instead, Federated Learning (FL) could be fruitfully leveraged in this scenario, since it provides for each satellite to train a local model only with its own dataset, and then to share its trained model with a central server, which receives models trained by the different satellites and aggregates them into a new global model being eventually shared with all the satellites, and this repeats until convergence is reached. However, because communication with a node acting as a central parameter server may be still limited by short visibility time, the described process may need a long time because of limited communication windows, negatively impacting the time needed to reach model convergence. For this reason, we propose a communication strategy to support a completely distributed learning technique to train a deep learning model in-orbit, by leveraging the fact that satellites may form a network thanks to the potential availability of Inter-Satellite Links (ISLs) within and between orbital planes. Our proposal is different from a FL approach since we provide for each satellite to receive all the information needed to calculate an updated global model by itself, without leaning on a central parameter server. Numerical results show that distributed learning outperforms FL in number of learning rounds completed in the unit time, allowing for reaching validation accuracy convergence in a shorter time, as it has been verified on a land coverage classification task based on the EuroSAT dataset.\nElsevier",
        "bibid": "valente2024proposalu54"
    },
    {
        "title": "GEO and LEO Collaborative Architecture Evolution in Future Integrated Satellite-Terrestrial Network",
        "abstract": "With the developing of communication technologies and capabilities from the ground to the space, the integration architecture of mobile communication network and the satellite network have become the key research prospect of the future 6G network. This paper investigates the current development status of Integrated Satellite-Terrestrial Network (ISTN), introduce the potential Geostationary Earth Orbit (GEO) and Low Earth Orbit (LEO) collaborative architecture evolution of ISTN architecture. Based on basic capabilities of the architecture such as control-forwarding separation, inter-satellite communication, network function and service on-boarding, three proposed networking schemes are shown. Also, two main key issue, including collaboration of networking schemes and selection of multiple access types, need to be solved. At the end, this paper indicates challenges the architecture brings and corresponding future work plan.\nieeexplore.ieee.org",
        "bibid": "wang2023geowuo"
    },
    {
        "title": "Multi-Satellite Cooperative Computing Task Offloading Strategy Based on Deep Reinforcement Learning",
        "abstract": "With the proposition of an integrated space-air-ground-sea communication network concept, satellites have become a key bridge connecting the globe. Low Earth Orbit (LEO) satellites, in particular, have become an important force in achieving global seamless communication due to their low latency and wide coverage advantages. LEO satellite networks can provide computing services to ground users under extreme conditions, which is of significant practical importance. The coordination among satellite edge nodes not only improves the utilization of space-based resources but is also key to enhancing the QoS of satellite edge computing services. Therefore, we propose a computation offloading strategy based on deep reinforcement learning (DRL) for multi-satellite cooperative computing scenar-ios (DQN-SCCO). First, the optimization problem is formulated as a Markov decision process (MDP), and the optimal solution to the problem is approached through a deep Q-network (DQN). Simulation results show that DQN-SCCO can effectively reduce the task set response delay and improve the task offloading success rate compared to baseline algorithms.\nieeexplore.ieee.org",
        "bibid": "cao2024multin84"
    },
    {
        "title": "[HTML] Analysing the synergies between Multi-agent Systems and Digital Twins: A systematic literature review",
        "abstract": "Context\n: Digital Twins (DTs) are used to augment physical entities by exploiting assorted computational approaches applied to the virtual twin counterpart. A DT is generally described as a physical entity, its virtual counterpart, and the data connections between them. Multi-Agent Systems (MAS) paradigm is alike DTs in many ways. Agents of MAS are entities operating and interacting in a specific environment, while exploring and collecting data to solve some tasks.\nObjective\n: This paper presents the results of a systematic literature review (SLR) focused on the analysis of current proposals exploiting the synergies of DTs and MAS. This research aims to synthesize studies that focus on the use of MAS to support DTs development and MAS that exploit DTs, paving the way for future research.\nMethod\n: A SLR methodology was used to conduct a detailed study analysis of 64 primary studies out of a total of 220 studies that were initially identified. This SLR analyses three research questions related to the synergies between MAS and DT.\nResults\n: The most relevant findings of this SLR and their implications for further research are the following: i) most of the analyzed proposals design digital shadows rather than DT; ii) they do not fully support the properties expected from a DT; iii) most of the MAS properties have not fully exploited for the development of DT; iv) ontologies are frequently used for specifying semantic models of the physical twin.\nConclusions\n: Based on the results of this SLR, our conclusions for the community are presented in a research agenda that highlights the need of innovative theoretical proposals and design frameworks that guide the development of DT. They should be defined exploiting the properties of MAS to unleash the full potential of DT. Finally, ontologies for machine learning models should be designed for its use in DT.\nElsevier",
        "bibid": "pretel2024analysingf05"
    },
    {
        "title": "ePMLF: Efficient and Privacy‐Preserving Machine Learning Framework Based on Fog Computing",
        "abstract": "With the continuous improvement of computation and communication capabilities, the Internet of Things (IoT) plays a vital role in many intelligent applications. Therefore, IoT devices generate a large amount of data every day, which lays a solid foundation for the success of machine learning. However, the strong privacy requirements of the IoT data make its machine learning very difficult. To protect data privacy, many privacy‐preserving machine learning schemes have been proposed. At present, most schemes only aim at specific models and lack general solutions, which is not an ideal solution in engineering practice. In order to meet this challenge, we propose an efficient and privacy‐preserving machine learning training framework (ePMLF) in a fog computing environment. The ePMLF framework can let the software service provider (SSP) perform privacy‐preserving model training with the data on the fog nodes. The security of the data on the fog nodes can be protected and the model parameters can only be obtained by SSP. The proposed secure data normalization method in the framework further improves the accuracy of the training model. Experimental analysis shows that our framework significantly reduces the computation and communication overhead compared with the existing scheme.\nWiley Online Library",
        "bibid": "zhao2023epmlfubd"
    },
    {
        "title": "Federated Reinforcement Learning with Constellation Collaboration for Dynamic Laser Inter-Satellite Link Scheduling",
        "abstract": "The scheduling of the laser inter-satellite links (LISLs) can effectively improve the network performance but is challenging in mega-constellations due to the large amount of satellites. In this paper, inter-OP LISL scheduling is realized by federated reinforcement learning, which reduces complexity by decomposing global scheduling into independent decisions for each satellite and solved by multi-agent reinforcement learning. Based on the constellations collaboration, federated learning (FL) with partial and global aggregations is utilized to reduce the overhead for the model update, where the geostationary orbit (GEO)-GEO collaboration for global aggregation can be performed infrequently since the indirect associations among the GEO satellites according to the motion of low Earth orbit (LEO) satellite. Meanwhile, simulation results indicate that the proposed method reduces the average hop count by over two hops and decreases the number of LISLs by over 25% compared to the fixed LISLs, and the model update overhead is reduced to 46.6% of that in the centralized method.\nieeexplore.ieee.org",
        "bibid": "wang2024federatedk7a"
    },
    {
        "title": "Fault Tolerance Oriented SFC Optimization in SDN/NFV-enabled Cloud Environment Based on Deep Reinforcement Learning",
        "abstract": "In software defined network/network function virtualization (SDN/NFV)-enabled cloud environment, cloud services can be implemented as service function chains (SFCs), which consist of a series of ordered virtual network functions. However, due to fluctuations of cloud traffic and without knowledge of cloud computing network configuration, designing SFC optimization approach to obtain flexible cloud services in dynamic cloud environment is a pivotal challenge. In this paper, we propose a fault tolerance oriented SFC optimization approach based on deep reinforcement learning. We model fault tolerance oriented SFC elastic optimization problem as a Markov decision process, in which the reward is modeled as a weighted function, including minimizing energy consumption and migration cost, maximizing revenue benefit and load balancing. Then, taking binary integer programming model as constraints of quality of cloud services, we design optimization approaches for single-agent double deep Q-network (SADDQN) and multi-agent DDQN (MADDQN). Among them, MADDQN decentralizes training tasks from control plane to data plane to reduce the probability of single point of failure for the centralized controller. Experimental results show that the designed approaches have better performance. MADDQN can almost reach the upper bound of theoretical solution obtained by assuming a prior knowledge of the dynamics of cloud traffic.\nieeexplore.ieee.org",
        "bibid": "chen2024faultonz"
    },
    {
        "title": "A joint strategy for service deployment and task offloading in satellite–terrestrial IoT",
        "abstract": "In recent years, low earth orbit satellite constellations, which are an important component of 6G, have been considered as a potential solution to achieve seamless network services for remote areas. Service deployment based on network function virtualization (NFV) has become an essential trend among satellite networks to enable flexible network services. However, current satellite–terrestrial IoT task offloading schemes rarely consider NFV-based satellite service deployment, which limits the performance of satellite networks. In this study, we address this problem by proposing an optimization problem that jointly considers service deployment and task offloading. To solve such a problem with many coupling decision variables, we decouple the problem using a two-stage approach. We propose a deep reinforcement learning-based service deployment policy to solve the service deployment subproblem and an alternating direction multiplier method-based distributed approach to solve the task offloading subproblem, which with the aim of minimizing the task latency and energy consumption of IoT devices. Simulation experiments demonstrate that our scheme can obtain near-optimal solution and can be adapted to large-scale satellite–terrestrial IoT network scenarios.\nElsevier",
        "bibid": "sun2023jointc74"
    },
    {
        "title": "Multi-objective optimisation strategy for on-orbit fault-tolerant decision making",
        "abstract": "With an increasing number of satellites in orbit, consensus across a heterogeneous group of satellites can lead to a more neutral, unbiased, and accurate decisions. Fault tolerant consensus algorithms such as Practical Byzantine Fault Tolerance (pBFT) require communication with all other network members up to 4 times. In a network with thousands of satellites in space on different trajectories, this time can approach millennia. There-fore, identifying a subset of satellites that can form a sub-network able to converge to a consensus decision in a useful time window, while maximising the number of members to increase consensus accuracy and trustworthiness, can be formulated as a multi-objective combinatorial optimisation problem. The problem is explained and defined with the optimisation method and the consensus algorithm steps described. Metrics for measuring the output of the optimal pareto front are considered and applied to the front computed. The real satellite positions used generate a non-fixed topology and high latency scenario such as that of a real on-orbit decision being made. The trend shown over 100 days of satellite positions propagation with up to 82 International Charter: Space and Major Disasters satellites shows up to 22 satellites can be used in a subset with a near linear increase in consensus time along the optimal pareto front and exponential trend for the mean values computed over 100 runs of the NSGA-II algorithm. The minimum consensus time is found to be 47 minutes for a subset of 4 satellites for the given time frame.\nieeexplore.ieee.org",
        "bibid": "cowlishaw2024multixsk"
    },
    {
        "title": "Artificial Intelligence in 6G Wireless Networks: Opportunities, Applications, and Challenges",
        "abstract": "Wireless technologies are growing unprecedentedly with the advent and increasing popularity of wireless services worldwide. With the advancement in technology, profound techniques can potentially improve the performance of wireless networks. Besides, the advancement of artificial intelligence (AI) enables systems to make intelligent decisions, automation, data analysis, insights, predictive capabilities, learning, and adaptation. A sophisticated AI will be required for next‐generation wireless networks to automate information delivery between smart applications simultaneously. AI technologies, such as machines and deep learning techniques, have attained tremendous success in many applications in recent years. Hances, researchers in academia and industry have turned their attention to the advanced development of AI‐enabled wireless networks. This paper comprehensively surveys AI technologies for different wireless networks with various applications. Moreover, we present various AI‐enabled applications that exploit the power of AI to enable the desired evolution of wireless networks. Besides, the challenges of unsolved research in this area, which represent the future research trends of AI‐enabled wireless networks, are discussed in detail. We provide several suggestions and solutions that help wireless networks be more intelligent and sophisticated to handle complicated problems. In summary, this paper can help researchers deeply understand the up‐to‐the‐minute wireless network designs based on AI technologies and identify interesting unsolved issues to be pursued in their research in a fast way.\nWiley Online Library",
        "bibid": "alhammadi2024artificialulq"
    },
    {
        "title": "Cooperative Federated Learning over Hybrid Terrestrial and Non-Terrestrial Networks",
        "abstract": "While network coverage maps continue to expand, many devices located in remote areas remain unconnected to terrestrial communication infrastructures, preventing them from getting access to the associated data-driven services. In this paper, we propose a cooperative ground-to-satellite federated learning (FL) methodology to facilitate machine learning service management over remote regions. Our methodology orchestrates satellite constellations to provide the following key functions during FL: (i) processing data offloaded from ground devices, (ii) aggregating models within device clusters, and (iii) relaying models/data to other satellites via inter-satellite links (ISLs). Due to the limited coverage time of each satellite over a particular remote area, we facilitate satellite transmission of trained models and acquired data to neighboring satellites via ISL, so that the incoming satellite can continue FL for the region. We also develop a training latency minimizer which optimizes over the amount of data to be offloaded from ground devices to satellites. Through experiments on benchmark datasets, we show that our scheme can significantly speed up the convergence of FL compared with terrestrial-only and other satellite baseline approaches.\nieeexplore.ieee.org",
        "bibid": "han2024cooperative1n6"
    },
    {
        "title": "DTFL: A Digital Twin-assisted Graph Neural Network Approach for Service Function Chains Failure Localization",
        "abstract": "Cloud computing enables Network Function Virtualization to dynamically provide and deploy network functions (NFs) to meet business-specific requirements. This approach streamlines NFs’ lifecycle management and lowers the cost of Operation Administration and Maintenance. However, these advantages cause Service Function Chain (SFC) failure to grow in both scope and dimensionality, making it difficult to establish a model to locate the failure effectively. In this paper, we propose a complete analysis scheme DTFL (Digital Twin (DT) based for SFC failure localization (FL)) through the following two steps: one is classifying and locating failures, and the other is conducting root cause analysis. We propose transGNN based on the Graph Neural Network and improved graph search model to achieve the classification and location for SFC failures. On this basis, the FNSG-RCA algorithm (failure based graph model) is proposed to analyze failures. We build a prototype based on the cloud platform and experimental results show that this scheme can achieve an accuracy rate of over 98% in fine-grained classification of 49 failure types. In addition, DTFL delivers desirable performance in RCA, approximately 13% more accurate than SOTA, the state-of-the-art approach. DTFL improves both RCA accuracy and model deployment efficiency compared with the non-DT approaches.\nieeexplore.ieee.org",
        "bibid": "guo2023dtflxc7"
    },
    {
        "title": "A knowledge service framework for fault diagnosis of low-earth orbit satellite constellation",
        "abstract": "The rapid expansion of low-earth orbit satellite constellations poses a significant challenge for the operation and maintenance of thousands of satellites. In-orbit fault diagnosis helps minimize the cost of repairs, and ensure the overall reliability of the satellite constellation by identifying faults in their early stages. Compared to traditional fault detection methods, fault diagnosis requires knowledge to enhance very limited data and to infer cascading failures. In this paper, we propose a novel knowledge service framework that combines data-driven and knowledge-driven models to improve the efficiency and effectiveness of fault diagnosis. We implement a fault diagnosis service based on a constructed knowledge graph, which affords an assistant decision support function. The in-orbit deployment and simulation experiment verify the feasibility of the proposed framework, which shows great potential for fault diagnosis of low-earth orbit satellite constellation.\nieeexplore.ieee.org",
        "bibid": "teng2023knowledgeodv"
    },
    {
        "title": "Stigmergy and Hierarchical Learning for Routing Optimization in Multi-domain Collaborative Satellite Networks",
        "abstract": "The integration of Software-Defined Networking (SDN) and Artificial Intelligence (AI) presents promising opportunities for managing and optimizing LEO satellite network routing. However, as the scale and coverage of satellite networks continue to expand, challenges are posed to both centralized and distributed architectures in terms of managing network information and coping with routing complexity. To overcome these challenges, leveraging distributed SDN technology, a stigmergy multi-agent hierarchical deep reinforcement learning routing algorithm is proposed in multi-domain collaborative satellite networks. A pheromone-based mechanism is incorporated to facilitate collaboration during independent training, and hierarchical control is employed to decouple the complexity of cross-domain routing decisions. Simulation results demonstrate that our proposed algorithm exhibits good scalability and performance in large-scale satellite networks.\nieeexplore.ieee.org",
        "bibid": "li2024stigmergyrgk"
    },
    {
        "title": "ePMLF: Efficient and Privacy‐Preserving Machine Learning Framework Based on Fog Computing",
        "abstract": "With the continuous improvement of computation and communication capabilities, the Internet of Things (IoT) plays a vital role in many intelligent applications. Therefore, IoT devices generate a large amount of data every day, which lays a solid foundation for the success of machine learning. However, the strong privacy requirements of the IoT data make its machine learning very difficult. To protect data privacy, many privacy‐preserving machine learning schemes have been proposed. At present, most schemes only aim at specific models and lack general solutions, which is not an ideal solution in engineering practice. In order to meet this challenge, we propose an efficient and privacy‐preserving machine learning training framework (ePMLF) in a fog computing environment. The ePMLF framework can let the software service provider (SSP) perform privacy‐preserving model training with the data on the fog nodes. The security of the data on the fog nodes can be protected and the model parameters can only be obtained by SSP. The proposed secure data normalization method in the framework further improves the accuracy of the training model. Experimental analysis shows that our framework significantly reduces the computation and communication overhead compared with the existing scheme.\nWiley Online Library",
        "bibid": "zhao2023epmlf3vu"
    },
    {
        "title": "Space Systems, Quantum Computers, Big Data and Sustainability: New Tools for the United Nations Sustainable Development Goals",
        "abstract": "The 17 Sustainable Development Goals for 2030 of the United Nations have well defined quantitative objectives in each of these different areas. The ability to note significant progress against these goals, in order to determine if advancement is truly being achieved, is heavily dependent on space sensing systems. Likewise, space systems are also frequently key to detecting problems, enabling progress, and indeed crucial to achieving these goals. New high performance space systems plus enhanced data processing systems, including big data analysis and artificially intelligence, can provide a ‘real-time’ understanding of global activities—both positive and negative. The petabytes of data generated by advanced multi-spectral Earth Observation Systems are not the essence of these space systems of the future. The prime factor is the advanced intelligence and accelerated data processing techniques that can, in near real-time, discover environmental or legal infractions, buildings or bridges in danger of collapse, the spread of fires or tree or plant infections, changes in demographics, and more. They can also determine progress toward positive goals. This chapter will explore how space systems linked to Big Data Analysis and AI interpretative systems can support efforts to achieve the United Nations’ Sustainable Development Goals, enable enforcement of environmental laws and regulations, and support positive global development in new ways that are much more rapid, accurate, and actionable. “The authors wish to particularly thank Peter Marshall for his help in reviewing and editing this chapter.”\ntaylorfrancis.com",
        "bibid": "pelton2024spaceq9u"
    },
    {
        "title": "Edge computing offloading strategy for space-air-ground integrated network based on game theory",
        "abstract": "The limited coverage of terrestrial networks makes it difficult to provide low-latency and high-reliable computing services for users and Internet of Thing (IoT) devices in remote areas such as mountainous regions and oceans. Space-Air-Ground Integrated Network (SAGIN) combined with Mobile Edge Computing (MEC) can provide seamless three-dimensional services for users by deploying edge servers on satellites, Unmanned Aerial Vehicles (UAVs) and ground infrastructures. However, due to the limited heterogeneous computing resources in satellites-UAV clusters network and the energy resources of IoT devices, it brings a significant challenge in determining how to offload the computing tasks generated by ground user devices to satellite edge nodes, UAV edge nodes or locally for processing. In this paper, we first propose a satellites-UAV clusters-ground three-layer edge computing network architecture consisting of a global controller, inter-domain controllers and MEC servers. We then model the task offloading problem as a Binary Integer Linear Programming (BILP) aiming at minimizing the offloading cost composed of delay and energy consumption and prove it is NP-hard. Next, the original offloading problem is transformed to a noncooperative strategic game and the existence of Nash equilibrium is proven using potential games. Finally, we propose a Nash Equilibrium Iteration Offloading algorithm based on Game theory (NEIO-G) to find the optimal offloading strategy. Compared with other baseline algorithms, simulation results demonstrate that the NEIO-G can significantly reduce the system offloading overhead in terms of delay and energy consumption.\nElsevier",
        "bibid": "liu2024edgehcy"
    },
    {
        "title": "Online Energy-Efficient Resource Allocation in Integrated Terrestrial and Satellite 6G Networks",
        "abstract": "In this paper, we jointly study the real-time user association, traffic routing and x-Network Function (xNF) placement problem in an integrated Terrestrial and Satellite 6G Network (TN-SN), with the aim of maximizing the network energy efficiency and user acceptance ratio. We formulate the aforementioned problem as a Mixed Integer Linear Program (MILP), considering various capacity, power and flow conservation constraints for the integrated network, while also meeting the specific service requirements of each user. To tackle the increased complexity of the optimal solution, we also develop an efficient heuristic (named as TERA). Through extensive simulations, TERA demonstrates a notable superiority in energy efficiency compared to the current State-of-the-Art (SoA), achieving up to 85 % of the optimal with up to 87 % lower execution time even under challenging traffic load conditions.\nieeexplore.ieee.org",
        "bibid": "mesodiakaki2024onlinefra"
    },
    {
        "title": "Application of artificial intelligence technology in LEO satellite communications",
        "abstract": "Satellite communication has the advantages of wide coverage and large capacity, and is an important means to achieve cross-domain information transmission. The article introduces the current situation of the development of LEO satellite communications and the main challenges it faces, combines the characteristics of artificial intelligence technology, focuses on the analysis of the application mode of artificial intelligence in the scenarios of communication anti-jamming, network resource optimization, and the integration of heaven and earth, and looks forward to the trend of the intelligent development in the field of LEO satellite communications.\nACM Digital Library",
        "bibid": "li2024application1bj"
    },
    {
        "title": "DoSat: A DDoS Attack on the Vulnerable Time-Varying Topology of LEO Satellite Networks",
        "abstract": "Low Earth orbit (LEO) satellite networks, which feature low-latency and full-coverage connectivity, promise to revolutionize the Internet and become an indispensable part of the next-generation communications network. However, due to the limited bandwidth and processing resources available on board, LEO satellite networks are susceptible to network attacks, especially link flooding attacks (LFAs). LFAs are a specific type of the notorious DoS attack where the attacker tries to cut off critical network links using seemingly legitimate traffic. Unlike attacks targeted directly on servers, LFAs undermine networks in a more insidious manner. In this paper, we present DoSat (DDoS on Satellites), an LFA attack model that focuses on the time-varying topology of satellite networks. The model takes advantage of such an opportunity to concentrate attack traffic: the traffic having been sent out during the process of path delay switching will reach the destination in tandem. We demonstrate through simulation experiments that DoSat can reduce the cost of LFAs by approximately 20% without any tradeoffs of attack’s undetectability.\nSpringer",
        "bibid": "lu2024dosat58z"
    },
    {
        "title": "Satellite-assisted edge computing management based on deep reinforcement learning in industrial internet of things",
        "abstract": "The insufficient edge computing equipment in remote areas cannot meet explosively growing computing needs of industrial Internet of things devices, which undoubtedly leads to unaffordable overheads of the device-side energy and computation timeout. In response to this problem, we propose a collaborative computation offloading architecture based on the satellite-assisted edge computing (SAEC) deployed in low earth orbit ultra-dense satellite networks which hold great promise in the 6G communications benefiting from the low latency, high bandwidth, global coverage, etc. To make the non-differentiable computation offloading problem tractable, we propose an asynchronous advantage actor-critic based SAEC offloading (ASO) deep reinforcement learning (DRL) algorithm to optimize the integrated reward reflecting in latency and energy, and optimally train the action set determining the scale of participating SAEC servers and the distribution of their computational tasks. Numerous simulation results verify that our ASO algorithm can greatly accelerate the convergence and improve the integrated reward compared with contrast methods.\nElsevier",
        "bibid": "zhu2023satellitejjs"
    },
    {
        "title": "Dual-timescales Optimization for Resource Slicing and Task Scheduling in Satellite Edge Computing Networks",
        "abstract": "This paper establishes a dual-timescale framework for joint resource slicing and task scheduling in satellite edge computing (SEC) networks. Specifically, to capture network dynamics and task stochasticity at small timescales, we formulate the task scheduling problem as a Markov decision process (MDP) to minimize task delay, network energy consumption, and packet loss. We design a deep reinforcement learning-assisted task scheduling (DRTS) algorithm inspired by the soft actor-critic (SAC) algorithm to learn the scheduling policy. Task processing performance is affected by communication and computing re-sources allocated to respective resource slices. Thus, considering that frequent resource slicing has a significant management over-head, we further optimize resource slices on a larger timescale. To obtain a policy with low complexity, we propose a greedy-based heuristic algorithm. A hierarchical solution is constructed to find the optimal solution due to the correlation between the two timescale problems. Finally, to validate the effectiveness and superiority of the proposed scheme, extensive simulations are performed.\nieeexplore.ieee.org",
        "bibid": "fang2024dualyo5"
    },
    {
        "title": "[CITATION] IoT Computing Collaboration and Data-aware Routing Algorithm for Edge Computing and DL",
        "abstract": "With the development of mobile edge computing and neural network Deep Learning (DL), more and more scholars are studying the combination of the two. This paper mainly studies the application of mobile edge computing and neural network DL in IoT computing collaboration and data-aware routing algorithms. Therefore, this paper proposes the deployment options of MEC technology and ETSIMEC in mobile edge computing, combining mobile edge computing with DL. Designing an optimization algorithm based on Markov decision process and feature expression learning, and then analyze and optimize for IoT computing and VANET routing algorithm. In order to have a clearer direction for the optimization algorithm, this paper also designs the edge computing model training and experiment comparison, the DL algorithm comparison experiment, and the routing algorithm simulation experiment and performance analysis. Combined with the experimental results, it is optimized and compared with traditional IoT computing and routing algorithms. Finally, it is concluded that the computing efficiency of IoT computing based on edge computing and DL designed in this paper is 21.33% higher than that of traditional IoT computing. The efficiency of the routing algorithm based on edge computing and DL designed in this paper is 9.29% higher than that of the traditional routing algorithm.\nWorld Scientific",
        "bibid": "li2024iot28j"
    },
    {
        "title": "[PDF] Satellite-Air-Terrestrial Cloud Edge Collaborative Networks: Architecture, Multi-Node Task Processing and Computation.",
        "abstract": "Integrated satellite-terrestrial network (ISTN) has been considered a novel network architecture to achieve global three-dimensional coverage and ultra-wide area broadband access anytime and anywhere. Being a promising paradigm, cloud computing and mobile edge computing (MEC) have been identified as key technology enablers for ISTN to further improve quality of service and business continuity. However, most of the existing ISTN studies based on cloud computing and MEC regard satellite networks as relay networks, ignoring the feasibility of directly deploying cloud computing nodes and edge computing nodes on satellites. In addition, most computing tasks are transferred to cloud servers or offloaded to nearby edge servers, the layered design of integrated satellite-air-terrestrial architecture and the cloudedge-device cooperative processing problems have not been fully considered. Therefore, different from previous works, this paper proposed a novel satelliteair-terrestrial layered architecture for cloud-edge-device collaboration, named SATCECN. Then this paper analyzes the appropriate deployment locations of cloud servers and edge servers in ISTN, and describes the processing flow of typical satellite computing tasks. For computing resource allocation problems, this paper proposed a device-edge-cloud Multi-node Cross-layer Collaboration Computing (MCCC) method to find the optimal task allocation strategy that minimizes the task completion delay and the weighted system energy consumption. Furthermore, the approximate optimal solutions of the optimization model are obtained by using successive convex approximation algorithm, and the outstanding advantages of the proposed method in reducing system energy consumption and task execution delay are verified through experiments. Finally, some potential issues and directions for future research are highlighted.\ncdn.techscience.cn",
        "bibid": "liu2023satellite1m2"
    },

    {
        "title": "Multi-agent Deep Reinforcement Learning Aided Computing Offloading in LEO Satellite Networks",
        "abstract": "Legacy computing offloading approaches are originally designed for the terrestrial networks with rather static topologies, and may not be appropriate for the next-generation LEO satellite broadband networks (LSBNs) featured with high dynamicity. This paper presents a multi-agent deep reinforcement learning (MADRL) algorithm for making edge computing multi-level offloading decisions in the LSBNs. Particularly, computing offloading is formulated as a partially observable Markov decision process (POMDP) based multi-agent decision problem. Each LEO satellite is an intelligent agent, either conducting a received edge computing task or forwarding it to its four neighboring satellite or the nearest cloud node on the ground. These agents are fully cooperative and their deep neural network models used to make offloading decisions share the same parameter values and are trained by the same replay buffer. A centralized training and distributed execution framework is utilized to ensure that globally optimized offloading decisions can be achieved based on local observations. Comparative simulation experiments for six representative offloading approaches show that the proposed MADRL aided approach outperforms the others regarding to decreasing edge computing task processing delay and increasing onboard compute resource utilization ratio. In addition, the convergence of this MADRL aided approach is also the best among the three DRL-based approaches.\nieeexplore.ieee.org",
        "bibid": "lai2023multilvl"
    },
    {
        "title": "Internet of Senses-Potential Applications and Implications",
        "abstract": "The Internet of Senses (IoS) is an emerging field that aims to enhance human-machine interaction by enabling individuals to experience the digital world with their senses. This article, which explores a highly novel research topic, is at the forefront of Ericsson engineers' investigations, providing pioneering insights into the subject matter.IoS employs technologies such as virtual and augmented reality, haptic feedback, and olfactory and gustatory systems to provide multi-sensory experiences. This article provides an overview of the latest trends and innovations in IoS, highlighting its potential for human well-being and progress as well as the challenges that need to be addressed to ensure its safe and ethical implementation. The article also emphasizes the role of 6G in enabling IoS and the potential benefits of incorporating the chemical senses into digital technology. Overall, the IoS has the potential to revolutionize human-machine interaction and create immersive digital experiences.\ndergipark.org.tr",
        "bibid": "comert4internetllk"
    },
    {
        "title": "StarCross: Redactable blockchain-based secure and lightweight data sharing framework for satellite-based IoT",
        "abstract": "Due to limitations in network coverage and capacity, terrestrial networks cannot meet the growing demand for widespread intelligent applications in recent years. The increasingly popular Satellite-based Internet of Things (S-IoT), with its extensive and boundless coverage, has emerged as the optimal solution. However, the large-scale distributed communication and services have led to significant challenges in scalability and security. Blockchain can help mitigate security and privacy issues in data sharing of S-IoT. To address these challenges, we propose StarCross, a secure and lightweight data sharing framework for S-IoT based on redactable blockchain. Firstly, we develop a space-ground collaborative sharding blockchain network architecture. Low Earth Orbit (LEO) satellite network serves as the communication intermediary between shards. Secondly, StarCross employs an improved Proof of Work (PoW) consensus based on dynamic difficulty to mitigate the challenges of uneven computing power and centralization within each shard. To ensure the atomicity of cross-shard transactions, StarCross introduces a blockchain rewriting mechanism called RBCVC, which combines chameleon hash and voting-based consensus. Thirdly, we conducted theoretical analysis and experimental evaluations of StarCross. The results indicate that compared to existing solutions, StarCross achieves better scalability and security in S-IoT. StarCross’s TPS increased to 260.9% of the baseline, while the transaction confirmation latency decreased by 79.7%.\nElsevier",
        "bibid": "du2024starcrossfnr"
    },
    {
        "title": "面向空间分布式计算的动态任务分解及长时保障机制.",
        "abstract": "低轨卫星具有覆盖范围广, 离地面近等优势, 随着在轨处理能力的不断增强, 未来将成为地面网络的重要补充. 然而, 随着用户对网络服务实时性的需求日益增长, 如何在资源有限的条件下, 基于低轨卫星为用户提供计算密集型服务, 已成为一个急需解决的问题. 尤其是在低轨卫星高速移动, 星间链路动态切换的情况下, 如何保证空间计算能力能持续稳定地驻留在用户区域并提供稳定可靠的服务, 无疑是一项巨大的挑战. 为了解决上述问题, 提出一种动态任务分解聚合的分布式计算策略, 通过卫星分布式计算解决单星算力不足的问题. 在进行任务分解与调度时, 充分考虑卫星网络的资源占用情况以及子任务之间的关联关系, 对任务进行灵活的分解聚合. 此外, 为将低轨卫星算力驻留在用户区域, 解决低轨卫星服务周期短的问题, 研究并设计了一种长时保障机制. 根据实时卫星网络拓扑及任务分解调度图, 结合任务间的关联关系进行迁移决策, 对卫星迁移过程进行模块化设计, 根据实时网络状况调整迁移过程中的数据压缩率以及服务切换方式, 降低迁移过程中服务的中断时间. 仿真实验结果表明, 提出的策略可保障长时分布式计算, 能提供服务的平均时长延长了 110%, 用户满意度提高了约 20%, 迁移开销以及任务间的传输开销均降低了约 15%.\nsearch.ebscohost.com",
        "bibid": "锁啸天2024面向空间分布式计算的动态任务分解及长时保障机制wue"
    },
    {
        "title": "Deep Reinforcement Learning-Based Moving Target Defense for Multicast in Software-Defined Satellite Networks",
        "abstract": "The development of LEO satellite networks (LSN) makes them a potential solution to deliver broadcast/multicast traffic to deploy and upgrade massive amounts of Internet of Things (IoT) devices in future 6G networks. However, inherent resource constraints of LSN leave them vulnerable to a multitude of security threats, most notably distributed denial-of-service (DDoS) attacks. Existing solutions are primarily based on machine learning detection methods which are incapable of defending against unknown zero-day attacks. This paper presents an innovative solution leveraging deep reinforcement learning (DRL) to create a dynamic multicast tree based on moving target defense (MTD), aimed at enhancing the security of multicast services in LSN. The proposed solution adopts an adaptive orbital tree mutation (AOTM) scheme that dynamically adjusts multicast tree configurations considering quality of service (QoS) constraints to avoid attacks on vulnerable nodes. Simulations demonstrate the effectiveness of the AOTM scheme, showcasing its superior defense success rates compared to existing state-of-the-art algorithms.\nieeexplore.ieee.org",
        "bibid": "lian2024deepp5b"
    },
    {
        "title": "6G Wireless Communication Systems and Its Applications",
        "abstract": "In the coming years, additional iterations of the 5th generation (5G) of wireless communication will be introduced. However, due to the inherent constraints of 5G and the development of new applications and services with demanding specifications like latency, energy/bit, traffic capacity, and peak data rate, telecom researchers are now concentrating on conceptualizing the following generation of wireless communications, known as sixth-generation wireless communications (6G). The Internet of Things (IoT) is anticipated to transform consumer applications and services, ushering in a future of fully intelligent and autonomous systems leveraging sixth-generation networks. A collaborative effort between industry and academia has started to conceptualize the sixth generation of wireless communication systems with the aim of laying the groundwork for stratification of communication needs in the 2030s in order to meet these demanding requirements and maintain wireless networks’ competitive edge. This work also goes deeply into the challenges, demands, and trends related to 6G while also providing a future vision for 6G wireless communication and its network architecture. This work includes some fresh, intriguing 6G services and use cases based on the requirements and solutions, which cannot be adequately supported by 5G. Furthermore, this study provides information on key research directions that contribute to successful 6G conceptualization and implementation.\nSpringer",
        "bibid": "swetha20236gfzt"
    },
    {
        "title": "Ground Station Deployment Based on Data Center-User Gravity Model in Satellite-Terrestrial Integrated Networks",
        "abstract": "In recent years, research on satellite networks has gained significant attention, with their capability for seamless global coverage and meeting real-time communication demands serving as a key solution to address deficiencies in ground communication network coverage and to improve the real-time transmission of services. Traditional satellite networks, originally employed for singular purposes such as data relay, are gradually transitioning to satellite internet to support various Internet-based services. The integration of satellites with ground networks, known as the Satellite-Terrestrial Integrated Network (STIN), has become an inevitable trend, making the deployment of ground stations (GSs) a critical issue in the STIN construction. Traditional GS deployment strategies are insufficient to meet the real-time demands of emerging services. In this context, a GS deployment strategy based on the data center-user gravity model (GSD-DG) is proposed, where the influence of all data center factors on GS deployment is considered. This approach takes into account constraints such as satellite connectivity, user traffic, and data center gravity. The integration of GS with data centers plays a pivotal role in enhancing the internet service latency performance. Simulation results indicate that the proposed strategy significantly reduces service latency by 24.5% compared to the benchmark, providing a more effective GS deployment solution to further optimize the STIN service latency performance.\nieeexplore.ieee.org",
        "bibid": "zheng2024ground0n3"
    },
    {
        "title": "A Lightweight Hierarchical Mobility Management Architecture for Ultra-Dense LEO Satellite Network",
        "abstract": "As one of the most promising architecture in the evolving sixth-generation (6G) systems, ultra-dense low Earth orbit (LEO) satellite network (UD-LSN) is drawing increasing attention due to its global coverage and ubiquitous access. To ensure service continuity, mobility management with provision of seamless handover is crucial in the process of satellite and user movement. However, massive service requests and overlapped satellite coverage will result in frequent handovers and diversified options in the UD-LSN. Meanwhile, existing mobility management methods based on the terrestrial networks are difficult to make timely and effective decisions due to the limited deployments of ground stations. In light of this, we propose a two-layer grouping and clustering based mobility management architecture (GCMMA) for the UD-LSN to reduce the management complexity with supporting the flexible function configurations. Under the GCMMA, we design lightweight handover procedures for different scenarios according to the established handover model, which considers user aggregation and combines with the regularity of satellite motion. Simulation results validate the effectiveness of the proposed mechanism, which has a better performance in handover delays and signaling overheads.\nieeexplore.ieee.org",
        "bibid": "qin2023lightweightr19"
    },
    {
        "title": "Characterizing and analyzing leo satellite cyber landscape: A starlink case study",
        "abstract": "Ushering into the ‘New Space Era’, characterized by a reduction in launch expenses and simultaneous proliferation of commercial and governmental entities involved, the prominence of Low Earth Orbit (LEO) satellite technology in the sphere of Internet connectivity has risen to the forefront. However, due to current limitations under the overarching principle of ‘security-through-obscurity’, few to no research efforts have shed light on the intricacies of these networks. To this end, this paper harnesses a multilayer empirical approach in an effort to conduct an exploratory characterization and scrutiny of the cybersecurity landscape of Starlink, the largest LEO network. Using our built-in arsenal of data feeds, composed of large dark IP addresses, passive measurement sensors, BGP collectors, coupled with publicly available sources, we unveil on the Starlink cyberspace (i) Internet-scale exploitations, (ii) illicit scanning events originating from 8,675 unique Starlink end-users, (iii) suspicious Port 0 and IKE scans, (iv) Mirai-based infections, (v) source address spoofing, (vi) 8,714 vulnerabilities ranging between medium and critical, and (vii) interesting RTBH announcements associated with possible mitigation techniques.\nieeexplore.ieee.org",
        "bibid": "tieby2024characterizing609"
    },
    {
        "title": "Fairness-aware dynamic vnf mapping and scheduling in sdn/nfv-enabled satellite edge networks",
        "abstract": "Satellite edge computing (SEC) has emerged as a promising technology to deliver network services to remote users. Coupled with software-defined networking (SDN) and network function virtualization (NFV), SEC can provide flexibility, agility, and efficiency when allocating computing and storage resources. However, there still remain a number of technical challenges in terms of fairness and efficiency of the allocation of physical resources in service provisioning, especially in a satellite network with limited resources and dynamic traffic demands. In this paper, we investigate a dynamic virtual network function (VNF) mapping and scheduling in an SDN/NFV-enabled SEC environment to maximize the fairness between competing services in terms of the E2E delay safe margin to enhance the service acceptance rates in the network. We mathematically formulate the VNF mapping and scheduling problem as a nonlinear integer optimization problem, which is NP-hard. In order to effectively solve the problem, this paper proposes a two-stage heuristic dynamic VNF mapping and scheduling algorithm: i) the path selection algorithm returns all possible paths for a given service request with multiple VNFs, which are sorted in ascending order based on their E2E service delay and executed offline, and ii) the dynamic VNF mapping and scheduling algorithm performs online dynamic remapping and rescheduling of VNFs. Finally, numerical results are provided to demonstrate that the proposed algorithm offers a higher service acceptance rate, computing resource utilization efficiency, and higher fairness compared to a benchmark scheme.\nieeexplore.ieee.org",
        "bibid": "abreha2023fairnesscmd"
    },
    {
        "title": "Ace-Sniper: Cloud-Edge Collaborative Scheduling Framework With DNN Inference Latency Modeling on Heterogeneous Devices",
        "abstract": "The cloud–edge collaborative inference requires efficient scheduling of artificial intelligence (AI) tasks to the appropriate edge intelligence devices. Gls DNN inference latency has become a vital basis for improving scheduling efficiency. However, edge devices exhibit highly heterogeneous due to the differences in hardware architectures, computing power, etc. Meanwhile, the diverse deep neural networks (DNNs) are continuing to iterate over time. The diversity of devices and DNNs introduces high computational costs for measurement methods, while invasive prediction methods face significant development efforts and application limitations. In this article, we propose and develop Ace-Sniper, a scheduling framework with DNN inference latency modeling on heterogeneous devices. First, to address the device heterogeneity, a unified hardware resource modeling (HRM) is designed by considering the platforms as black-box functions that output feature vectors. Second, neural network similarity (NNS) is introduced for feature extraction of diverse and frequently iterated DNNs. Finally, with the results of HRM and NNS as input, the performance characterization network is designed to predict the latencies of the given unseen DNNs on heterogeneous devices, which can be combined into most time-based scheduling algorithms. Experimental results show that the average relative error of DNN inference latency prediction is 11.11%, and the prediction accuracy reaches 93.2%. Compared with the nontime-aware scheduling methods, the average waiting time for tasks is reduced by 82.95%, and the platform throughput is improved by 63% on average.\nieeexplore.ieee.org",
        "bibid": "liu2023acev7l"
    },
    {
        "title":"Review on 5G-satellite integrated network",
        "abstract":"Based on the reviews of standardizations and many other works of 5G-satellite integrated network (5GSIN), the research progress of 5GSIN key technologies, applications and network architectures were expounded and analyzed.Aiming at the key problems caused by the strong heterogeneity and large spatio-temporal scale of 5GSIN, the multi-layer distributed network architecture was proposed, and the architecture management method was designed.Finally, several key directions of the future development of 5GSIN were analyzed from the perspective of intelligence, in order to provide some references for research in related fields.",
        "bibid":"杨力20225g"
    },
    {
        "title":"Research on Strategies and Technologies for Resource Management and Control of Heterogeneous Network of High and Low Orbit Satellites",
        "abstract":"With the vigorous development of space communication technology and the continuous advancement of space-integrated-ground information network, satellite communication networks resource management and control is becoming more and more complex.Due to the scarcity of satellite resources, the slowness of resource scheduling compared to the status refreshing, and uneven distribution of business, eff ciently managing resources has become one of urgent problems to be solved in the development of satellite communications.In view of the heterogeneous network system architecture of high and low orbit satellites, the challenges, its network resource management and control facing, were analyzed.Integrating on the basis of traditional management and control architecture, the collaborative management and control architecture based on group management was introduced.The management strategy of satellite network virtual resource pool was explained to relieved resources scarcity.Resource scheduling algorithms based on deep reinforcement learning(DRL) was introduced to solved the mismatch problem of traditional scheduling methods in complex environments.Beam-hopping technology was adopted to deal with the two-dimensional unevenness of service distribution in time and space.",
        "bibid":"张美蓉2021高低轨卫星异构网络资源管控策略与技术研究"
    },   
    {
        "title":"Intelligent Resource Management for Satellite and Terrestrial Spectrum Shared Networking toward B5G",
        "abstract":"Integrated satellite-terrestrial networks (ISTNs) toward beyond fifth-generation (B5G) wireless systems benefiting from both satellite and terrestrial systems can achieve all-time seamless and broad coverage. Considering the scarcity of frequency resources and intense satellite-terrestrial cochannel interference, intelligent resource allocation with high spectrum efficiency and low co-channel interference has received a substantial amount of attention. Focusing on the spectrum efficiency advantages achieved by spectrum sensing and prediction, a hierarchical satellite and terrestrial spectrum shared framework based on the spectrum management unit (SMU) is proposed. Moreover, an intelligent resource management scheme in the SMU composed of spectrum sensing, prediction and allocation is formulated to improve spectrum efficiency with different user densities. We present a support vector machine (SVM) based algorithm that improves the accuracy and robustness of the learned model for the detection of spectrum occupancy. Then, a convolutional neural network (CNN) based spectrum prediction (SP) is performed, where the CNN is trained with the historical detection results from spectrum sensing. In addition, an intelligent resource management scheme including spectrum sensing, prediction and allocation based on the priorities and requirements of users is proposed to improve spectrum utilization. The evaluation results demonstrate that the proposed intelligent resource management scheme can achieve lower error detection probability and better spectrum efficiency.",
        "bibid":"jia2020intelligent"
    },
    {
        "title":"Integrated Resource Management for Terrestrial-Satellite Systems",
        "abstract":"As data traffic in terrestrial-satellite systems surges, the integration of power allocation for caching, computing, and communication (3C) has attracted much research attention. However, previous works on 3C power allocation in terrestrial-satellite systems mostly focus on maximizing the overall system throughput. In this paper, we aim to guarantee both throughput fairness and data security in terrestrial-satellite systems. Specifically, we first divide the system implementation into three steps, i.e., data accumulation, blockchain computing, and wireless transmission. Then, we model and analyze the delay and power consumption in each step by proposing several theorems and lemmas regarding 3C power allocation. Based on the theorems and lemmas, we further formulate the problem of 3C power allocation as a Nash bargaining game and construct an optimization model for the game. Last, we solve the optimization problem using dual decomposition and obtain the optimal period of the satellite serving the ground stations as well as the optimal 3C power allocation solution. The optimal solution can provide guidelines for parameter configuration in terrestrial-satellite systems. The performance of the proposed terrestrial-satellite architecture is verified by extensive simulations.",
        "bibid":"fu2020integrated"
    },
    {
        "title":"Radio Resource Management Techniques for Multibeam Satellite Systems",
        "abstract":"Next–generation of satellite communication (SatCom) networks are expected to support extremely high data rates for a seamless integration into future large satellite-terrestrial networks. In view of the coming spectral limitations, the main challenge is to reduce the cost (satellite launch and operation) per bit, which can be achieved by enhancing the spectral efficiencies. In addition, the capability to quickly and flexibly assign radio resources according to the traffic demand distribution has become a must for future multibeam broadband satellite systems. This letter presents the radio resource management problems encountered in the design of future broadband SatComs and provides a comprehensive overview of the available techniques to address such challenges. Firstly, we focus on the demand-matching formulation of the power and bandwidth assignment. Secondly, we present the scheduling design in practical multibeam satellite systems. Finally, a number of future challenges and the respective open research topics are described.",
        "bibid":"kisseleff2020radio"
    },
    {
        "title":"A Dynamic Resource Scheduling Scheme in Edge Computing Satellite Networks",
        "abstract":"The LEO satellite network has been a valuable architecture due to its characteristics of wide coverage and low transmission delay. Utilizing LEO satellites as edge computing nodes to provide reliable computing services for access terminals will be the indispensable paradigm of integrated space-air-ground network. However, the design of resource division strategy in edge computing satellite (ECS) is not easy, considering different accessing planes and resource requirements of terminals. Moreover, network topology, available resources and relative motion need to be analyzed comprehensively to establish ECS collaborative network for emergency situations. To address these problems, a three-layer network architecture combined with software defined networking (SDN) model is proposed to guide the inter-satellite link (ISL) connection and ECS resource scheduling. The advanced K-means algorithm (AKG) and breadth-first-search-based spanning tree algorithm (BFST) are provided to realize ECS resource division and ISL construction respectively. Simulation results show that the proposed dynamic resource scheduling scheme is feasible and effective.",
        "bibid":"wang2021dynamic"
    },
    {
        "title":"Joint Access and Backhaul Resource Management in Satellite-Drone Networks: A Competitive Market Approach",
        "abstract":"In this paper, the problem of user association and resource allocation is studied for an integrated satellite-drone network (ISDN). In the considered model, drone base stations (DBSs) provide downlink connectivity to ground users whose demand cannot be satisfied by terrestrial small cell base stations (SBSs). Meanwhile, a satellite system and a set of terrestrial macrocell base stations (MBSs) are used to provide resources for backhaul connectivity for both DBSs and SBSs. For this scenario, one must jointly consider resource management over satellite-DBS/SBS backhaul links, MBS-DBS/SBS terrestrial backhaul links, and DBS/SBS-user radio access links as well as user association with DBSs and SBSs. This joint user association and resource allocation problem is modeled using a competitive market setting in which the transmission data is considered as a good that is being exchanged between users, DBSs, and SBSs that act as “buyers”, and DBSs, SBSs, MBSs, and the satellite that act as “sellers”. In this market, the quality-of-service (QoS) is used to capture the quality of the data transmission (defined as good), while the energy consumption the buyers use for data transmission is the cost of exchanging a good. According to the quality of goods, sellers in the market propose quotations to the buyers to sell their goods, while the buyers purchase the goods based on the quotation. The buyers profit from the difference between the earned QoS and the charged price, while the sellers profit from the difference between earned price and the energy spent for data transmission. The buyers and sellers in the market seek to reach a Walrasian equilibrium, at which all the goods are sold, and each of the devices' profit is maximized. A heavy ball based iterative algorithm is proposed to compute the Walrasian equilibrium of the formulated market. Analytical results show that, with well-defined update step sizes, the proposed algorithm is guaranteed to reach one Walrasian equilibrium. Simulation results show that, at the achieved Walrasian equilibrium solution, the proposed algorithm can yield a two-fold gain in terms of the number of radio access links with a data rate of over 40 Mbps, and a three-fold gain in terms of the number of backhaul links with a data rate greater than 1.6 Gbps.",
        "bibid":"hu2020joint"
    },
    {
        "title":"Collaborative Multi-Resource Allocation in Terrestrial-Satellite Network Towards 6G",
        "abstract":"Terrestrial-satellite networks (TSNs) are envisioned to play a significant role in the sixth-generation (6G) wireless networks. In such networks, hot air balloons are useful as they can relay the signals between satellites and ground stations. Most existing works assume that the hot air balloons are deployed at the same height with the same minimum elevation angle to the satellites, which may not be practical due to possible route conflict with airplanes and other flight equipment. In this paper, we consider a TSN containing hot air balloons at different heights and with different minimum elevation angles, which creates the challenge of non-uniform available serving time for the communication between the hot air balloons and the satellites. Jointly considering the caching, computing, and communication (3C) resource management for both the ground-balloon-satellite links and inter-satellite laser links, our objective is to maximize the network energy efficiency. Firstly, by proposing a tapped water-filling algorithm, we schedule the traffic to relay among satellites according to the available serving time of satellites. Then, we generate a series of configuration matrices, based on which we formulate the relation between relay time and the power consumption involved in the relay among satellites. Finally, the collaborative resource allocation problem for TSN is modeled and solved by geometric programming with Taylor series approximation. Simulation results demonstrate the effectiveness of our proposed scheme.",
        "bibid":"fu2021collaborative"
    },
    {
        "title":"Machine Learning-Based Resource Allocation in Satellite Networks Supporting Internet of Remote Things",
        "abstract":"Satellite networks have been regarded as a promising architecture for supporting the Internet of remote things (IoRT) due to their advantages of wide coverage and high communication capacity in remote areas, which further promotes the development of the satellites for IoRT networks (SIoRTNs). The effectiveness of multi-dimensional resource collaboration has significant impacts on the IoRT data downloading performance. However, the environment ’ s dynamics, e.g., channel conditions and solar infeed process, are unknown in practical scenarios, which poses daunting challenges in making efficient utilization of multi-dimensional resources. Motivated by this fact, we model the joint resource scheduling and IoRT data scheduling problem with the aim of maximizing the amount of the IoRT data of the overall network by applying the model-free reinforcement learning framework. To overcome the limitations of traditional reinforcement learning algorithms, we propose several feature functions by investigating the natural attributes of the multi-dimensional resources of the SIoRTNs, and further exploit the concept of function approximation to approximate the expected downloaded IoRT data given the network state. Furthermore, we propose a state-action-reward-state-action (SARSA) based actor-critic reinforcement learning (SACRL) resource allocation strategy to achieve the optimal resource allocation and IoRT data scheduling with casual information at LEO satellites. Simulations validate the convergence property and the effectiveness of the proposed SACRL algorithm in terms of the amount of the downloaded IoRT data. Particularly, we investigate the impact of typical network parameters on network performance to further provide guidance for future SIoRTN system design.",
        "bibid":"zhou2021machine"
    },
    {
        "title":"Resource Management in Space-Air-Ground Integrated Vehicular Networks: SDN Control and AI Algorithm Design",
        "abstract":"With its potential versatility and reliability, the space-air-ground integrated vehicular network (SAGVN) is envisioned as a promising solution to deliver quality vehicular services anywhere at any time. This article proposes a software defined framework for SAGVN to achieve flexible, reliable, and scalable network resource management. First, key applications and research challenges in resource management are identified. Then we propose a hybrid and hierarchical SAGVN control architecture to balance the trade-off between system status acquisition and signaling overhead in different scenarios. Considering the dynamic networking environment with multi-dimensional resources and diverse services, it is challenging to make optimal resource management decisions in real time; thus, artificial intelligence (AI)-based engineering solutions are investigated to facilitate efficient network slicing, mobility management, and cooperative content caching and delivery. A trace-driven case study is presented to demonstrate the effectiveness of the proposed SAGVN framework with AI-based methods in increasing the SAGVN throughput performance.",
        "bibid":"wu2020resource"
    },
    {
        "title":"A Survey on Resource Management in Joint Communication and Computing-Embedded SAGIN",
        "abstract":"The advent of the 6G era aims for ubiquitous connectivity, with the integration of non-terrestrial networks (NTN) offering extensive coverage and enhanced capacity. As manufacturing advances and user demands evolve, space-air-ground integrated networks (SAGIN) with computational capabilities emerge as a viable solution for services requiring low latency and high computational power. Resource management within joint communication and computing-embedded SAGIN (JCC-SAGIN) presents greater complexity than traditional terrestrial networks. This complexity arises from the spatiotemporal dynamics of network topology and service demand, the interdependency of large-scale resource variables, and intricate tradeoffs among various performance metrics. Thus, a thorough examination of resource management strategies in JCC-SAGIN is crucial, emphasizing the role of non-terrestrial platforms with processing capabilities in 6G. This paper begins by reviewing the architecture, enabling technologies, and applications in JCC-SAGIN. Then, we offer a detailed overview of resource management modeling and optimization methods, encompassing both traditional optimization approaches and learning-based intelligent decision-making frameworks. Finally, we outline the prospective research directions in JCC-SAGIN.",
        "bibid":"chen2024survey"
    },
    {
        "title":"Deep reinforcement learning based joint edge resource management in maritime network",
        "abstract":"Due to the rapid development of the maritime networks, there has been a growing demand for computation-intensive applications which have various energy consumption, transmission bandwidth and computing latency requirements. Mobile edge computing (MEC) can efficiently minimize computational latency by offloading computation tasks by the terrestrial access network. In this work, we introduce a space-air-ground-sea integrated network architecture with edge and cloud computing components to provide flexible hybrid computing service for maritime service. In the integrated network, satellites and unmanned aerial vehicles (UAVs) provide the users with edge computing services and network access. Based on the architecture, the joint communication and computation resource allocation problem is modelled as a complex decision process, and a deep reinforcement learning based solution is designed to solve the complex optimization problem. Finally, numerical results verify that the proposed approach can improve the communication and computing efficiency greatly.",
        "bibid":"xu2020deep"
    },
    {
        "title":"Reinforcement Learning Based Capacity Management in Multi-Layer Satellite Networks",
        "abstract":"The development of satellite networks is drawing much more attention in recent years due to the wide coverage ability. Composed of geosynchronous orbit (GEO), medium earth orbit (MEO), and low earth orbit (LEO) satellites, the satellite network is a three-layer heterogeneous network of high complexity, for which comprehensive theoretical analysis is still missing. In this paper, we investigate the problem of capacity management in the three-layer heterogeneous satellite network. We first construct the model of the network and propose a low-complexity method for calculating the capacity between satellites. Based on the time structure of the time expanded graph, the searching space is greatly reduced compared to traditional augmenting path searching strategies, which can significantly reduce the computing complexity. Then, based on Q-learning, we proposed a long-term optimal capacity allocation algorithm to optimize the long-term utility of the system. In order to reduce the storage and computing complexity, a learning framework with low-complexity is constructed while taking the properties of satellite systems into account. Finally, we analyze the capacity performance of the three-layer heterogeneous satellite network and also evaluate the proposed algorithms with numerical results.",
        "bibid":"jiang2020reinforcement"
    },
    {
        "title":"Artificial intelligence for satellite communication: A review",
        "abstract":"Satellite communication offers the prospect of service continuity over uncovered and under-covered areas, service ubiquity, and service scalability. However, several challenges must first be addressed to realize these benefits, as the resource management, network control, network security, spectrum management, and energy usage of satellite networks are more challenging than that of terrestrial networks. Meanwhile, artificial intelligence (AI), including machine learning, deep learning, and reinforcement learning, has been steadily growing as a research field and has shown successful results in diverse applications, including wireless communication. In particular, the application of AI to a wide variety of satellite communication aspects has demonstrated excellent potential, including beam-hopping, anti-jamming, network traffic forecasting, channel modeling, telemetry mining, ionospheric scintillation detecting, interference managing, remote sensing, behavior modeling, space-air-ground integrating, and energy managing. This work thus provides a general overview of AI, its diverse sub-fields, and its state-of-the-art algorithms. Several challenges facing diverse aspects of satellite communication systems are then discussed, and their proposed and potential AI-based solutions are presented. Finally, an outlook of field is drawn, and future steps are suggested.",
        "bibid":"fourati2021artificial"
    },
    {
        "title":"Two-Layer Game Based Resource Allocation in Cloud Based Integrated Terrestrial-Satellite Networks",
        "abstract":"This paper investigates the cooperative transmission and resource allocation in cloud based integrated terrestrial-satellite networks, where a resource pool at the cloud acts as the integrated resource management and control center of the entire network. Considering the operator offers two levels of services of different quality of service (QoS) and price, we formulate a two-layer game based resource allocation problem to maximize the utility of the operator, which is composed of the Stackelberg game between the operator and users, the evolutionary game between all users, and the energy minimization problem. By solving the evolutionary game with replicator dynamics, the selections of users are obtained for any pricing strategy. Then, based on the service selections of users, the energy minimization problem is solved to allocate power and computation resources among users while satisfying the QoS constraints. By analyzing the evolution relationship between the utility and the pricing strategy, we eventually find the Stackelberg equilibrium point of the system, and obtain the optimal pricing as well as the resource allocation strategies for the operator. Finally, numerical results are provided to analyze the behavior of users in the game model, and evaluate the performance of the optimal pricing and resource allocation strategies.",
        "bibid":"fourati2021artificial"
    },
    {
        "title":"Realizing Intelligent Spectrum Management for Integrated Satellite and Terrestrial Networks",
        "abstract":"Nowadays both satellite and terrestrial networks are expanding rapidly to meet the ever-increasing demands for higher throughput, lower latency, and wider coverage. However, spectrum scarcity places obstacles in the sustainable development. To accommodate the expanding network within a limited spectrum, spectrum sharing is deemed as a promising candidate. Particularly, cognitive radio (CR) has been proposed in the literature to allow satellite and terrestrial networks to share their spectrum dynamically. However, the existing CR-based schemes are found to be impractical and inefficient because they neglect the difficulty in obtaining the accurate and timely environment perception in satellite communications and only focus on link-level coexistence with limited interoperability. In this paper, we propose an intelligent spectrum management framework based on software defined network (SDN) and artificial intelligence (AI). Specifically, SDN transforms the heterogenous satellite and terrestrial networks into an integrated satellite and terrestrial network (ISTN) with reconfigurability and interoperability. AI is further used to make predictive environment perception and to configure the network for optimal resource allocation. Briefly, the proposed framework provides a new paradigm to integrate and exploit the spectrum of satellite and terrestrial networks.",
        "bibid":"liang2021realizing"
    },
    {
        "title":"A Survey on Nongeostationary Satellite Systems: The Communication Perspective",
        "abstract":"The next phase of satellite technology is being characterized by a new evolution in non-geostationary orbit (NGSO) satellites, which conveys exciting new communication capabilities to provide non-terrestrial connectivity solutions and to support a wide range of digital technologies from various industries. NGSO communication systems are known for a number of key features such as lower propagation delay, smaller size, and lower signal losses in comparison to the conventional geostationary orbit (GSO) satellites, which can potentially enable latency-critical applications to be provided through satellites. NGSO promises a substantial boost in communication speed and energy efficiency, and thus, tackling the main inhibiting factors of commercializing GSO satellites for broader utilization. The promised improvements of NGSO systems have motivated this paper to provide a comprehensive survey of the state-of-the-art NGSO research focusing on the communication prospects, including physical layer and radio access technologies along with the networking aspects and the overall system features and architectures. Beyond this, there are still many NGSO deployment challenges to be addressed to ensure seamless integration not only with GSO systems but also with terrestrial networks. These unprecedented challenges are also discussed in this paper, including coexistence with GSO systems in terms of spectrum access and regulatory issues, satellite constellation and architecture designs, resource management problems, and user equipment requirements. Finally, we outline a set of innovative research directions and new opportunities for future NGSO research.",
        "bibid":"al2022survey"
    },
    {
        "title":"Satellite-Terrestrial Integrated Edge Computing Networks: Architecture, Challenges, and Open Issues",
        "abstract":"STN has been considered a novel network architecture to accommodate a variety of services and applications in future networks. Being a promising paradigm, MEC has been regarded as a key technology-enabler to offer further service innovation and business agility in STN. However, most of the existing research in MEC enabled STN regards a satellite network as a relay network, and the feasibility of tasks processing directly on the satellites is largely ignored. Moreover, the problem of multi-layer edge computing architecture design and heterogeneous edge computing resource co-scheduling, have not been fully considered. Therefore, different from previous works, in this article, we propose a novel architecture named STECN, in which computing resources exist in multi-layer heterogeneous edge computing clusters. The detailed functional components of the proposed STECN are discussed, and we present the promising technical challenges, including meeting QoE requirements, cooperative computation offloading, multi-node task scheduling, mobility management and fault/failure recovery. Finally, some potential research issues for future research are highlighted.",
        "bibid":"xie2020satellite"
    },
    {
        "title":"Virtual Network Function Placement in Satellite Edge Computing With a Potential Game Approach",
        "abstract":"Satellite networks, as a supplement to terrestrial networks, can provide effective computing services for Internet of Things (IoT) users in remote areas. Due to the resource limitation of satellites, such as in computing, storage, and energy, a computation task from an IoT user can be divided into several parts and cooperatively accomplished by multiple satellites to improve the overall operational efficiency of satellite networks. Network function virtualization (NFV) is viewed as a new paradigm in allocating network resources on-demand. Satellite edge computing combined with the NFV technology is becoming an emerging topic. In this paper, we propose a potential game approach for virtual network function (VNF) placement in satellite edge computing. The VNF placement problem aims to minimize the deployment cost for each user request, furthermore, we consider that a satellite network should provide computing services for as many user requests as possible. We formulate the VNF placement problem as a potential game to maximize the overall network payoff and analyze the problem by a game-theoretical approach. We implement a decentralized resource allocation algorithm based on a potential game (PGRA) to tackle the VNF placement problem by finding a Nash equilibrium. Finally, we conduct the experiments to evaluate the performance of the proposed PGRA algorithm. The simulation results show that the proposed PGRA algorithm can effectively address the VNF placement problem in satellite edge computing.",
        "bibid":"gao2022virtual"
    },
    {
        "title":"VNF-Based Service Provision in Software Defined LEO Satellite Networks",
        "abstract":"Low earth orbit (LEO) satellite networks will play important roles in the sixth generation (6G) communication system. Software defined network technique is a novel approach introduced to the LEO satellite networks to improve the resource flexibility and efficiency, forming the software defined LEO satellite networks (SDLSNs). How to efficiently allocate the resources of SDLSN to provide services for the terrestrial users is a key issue. Hence, in this work, we explore the service provision for SDLSN via virtual network functions (VNFs) orchestration on the software defined time-evolving graph. In view of the scarce, intermittent and unstable satellite-to-satellite (S2S) links, the problem is formulated to minimize the S2S resource consumption while satisfying the terrestrial tasks, which is in the form of integer linear programming. Since the problem is intractable by exhaustive search, we design a branch-and-price algorithm based on the coupling of Dantzig-Wolfe decomposition, column generation, and branch-and-bound to efficiently acquire the optimal solution. Further, to obtain a faster solution for practical usage, we further design an approximation algorithm for the subproblem and leverage the beam search to accelerate the pruning for the search tree. Finally, extensive simulations are conducted and the numerical results validate the effectiveness of the proposed schemes.",
        "bibid":"jia2021vnf"
    },
    {
        "title":"Resource Allocation in Terrestrial-Satellite-Based Next Generation Multiple Access Networks With Interference Cooperation",
        "abstract":"In this paper, an uplink non-orthogonal multiple access (NOMA) terrestrial-satellite network is investigated, where the terrestrial base stations (BSs) communicate with satellite by backhaul link, and user equipments (UEs) share spectrum resource of access link. Firstly, a utility function which consists of the achieved terrestrial user rate and cross-tier interference caused by terrestrial BSs to satellite is design. Thus, the optimization problem can be modeled by maximizing the system utility function while satisfying the varying backhaul rate and UEs’ quality of service (QoS) constraints. The optimization problem is highly non-convex and can not be solved directly. Thus, we decouple the original problem into user association sub-problem, bandwidth assignment sub-problem, and power allocation sub-problem. In user association sub-problem, an enhanced-caching, preference relation, and swapping based algorithm is proposed, where the satellite UEs are selected by the channel coefficient ratio. The terrestrial UEs association considers the both caching state and backhaul link. Then we derive the closed-form expression of the bandwidth assignment. In power allocation sub-problem, we convert the non-convex term of the target function into the convex one by the Taylor expansion, and solve the transformed convex problem by an iterative power allocation algorithm. Finally, a three-stages iterative resource allocation algorithm by joint considering the three sub-problems is proposed. Simulation results are discussed to show the effectiveness of the proposed algorithms.",
        "bibid":"zhang2022resource"
    },
    {
        "title":"Resource Allocation for NOMA Based Space-Terrestrial Satellite Networks",
        "abstract":"Non-orthogonal multiple access (NOMA) has been extensively studied to improve the performance of space-terrestrial satellite networks on account of the shortage of frequency band resources. In this paper, terrestrial network and satellite network synergistically provide complete coverage for ground users. A user association scheme on account of the channel gain and distance between the ground users and the BSs is proposed to identify the users to be associated by the BSs, and there is an upper limit for the number of users associated with each BS. Then calculate the channel condition ratio to select the users served by the satellite. The all BSs provide service for those unselected users, and the NOMA technology is applied to terrestrial network. Then, a user pairing scheme which maximize the minimum the ground user channel correlation coefficient is formulated to match the terrestrial users in a NOMA group. On account of multiple antennas equipped by the BSs and satellite, beamforming is performed among groups of BSs and among satellite users so as to reduce multi-user interference. In the power allocation scheme, we introduce the alternative direction method of multipliers (ADMM) algotithm so as to optimize system energy efficiency. In addition, the objective function is a non-convex function, so the Dinkelbach-style scheme is presented to convert non-convex function into the convex-form function. Eventually, the performance of the presented algorithm is simulated and compared with the existing NOMA-FTPA algorithm. The results indicate that the presented algorithm has high superiority in system energy efficiency and it can be applied to this network.",
        "bibid":"wang2020resource"
    },
    {
        "title":"Joint Offloading and Resource Allocation for Satellite Assisted Vehicle-to-Vehicle Communication",
        "abstract":"Satellite assisted vehicle-to-vehicle (V2V) communication can provide services for vehicles in depopulated areas, and it can be employed as an effective complementary component for terrestrial vehicular networks. Since the available communication and computing resource for satellites are scarce, task offloading, computing and communication resource allocation, which are coupled with each other, are critical issues for satellite assisted V2V communication. To tackle these problems, we formulate the joint offloading decision, computing and communication resource allocation problem for satellite assisted V2V communication as a mixed-integer nonlinear programming problem with minimum weighted-sum end-to-end latency, and we decouple it into two subproblems. First, the Lagrange multiplier method is adopted to obtain the optimal computing and communication resource allocation with fixed offloading decision. Then, the results of the resource allocation subproblem are fed into the offloading decision problem, which is formulated as a Markov decision process. To maximize the long-term reward of offloading decision, a deep reinforcement learning based method is adopted to learn the optimal offloading decision. Finally, the simulation results show that the proposed joint task offloading and resource allocation approach has superior performance compared with other schemes.",
        "bibid":"cui2020joint"
    },
    {
        "title":"Network Utility Maximization Resource Allocation for NOMA in Satellite-Based Internet of Things",
        "abstract":"High-throughput satellite (HTS) is viewed as a promising solution for the next generation of satellite-based Internet of Things (S-IoT). Considering that the onboard communication resources, such as power and storage, are limited, we formulate a joint network stability and resource allocation optimization problem to maximize the long-term network utility of a nonorthogonal multiple access (NOMA) S-IoT downlink system. First, we establish two virtual queues for both the data queueing and power expenditure. Then, a joint optimal problem can be formulated as a problem that optimizes the time average of network utility, which perfectly matches the Lyapunov optimization framework. Therefore, by taking into account the condition of successive interference cancellation decoding, we propose a practical solution under the Karush-Kuhn-Tucker (KKT) conditions, and further introduce an optimal solution by using the particle swarm optimization (PSO) algorithm for the joint resource allocation problem. The simulation results demonstrate that our joint optimization allocation schemes outperform the existing benchmark schemes.",
        "bibid":"jiao2020network"
    },
    {
        "title":"LEO Satellite Access Network (LEO-SAN) Toward 6G: Challenges and Approaches",
        "abstract":"With the rapid development of satellite communication technologies, the space-based access network has been envisioned as a promising complementary part of the future 6G network. Aside from terrestrial base stations, satellite nodes, especially the low-earth-orbit (LEO) satellites, can also serve as base stations for Internet access, and constitute the LEO-satellite access network (LEO-SAN). LEO-SAN is expected to provide seamless massive access and extended coverage with high signal quality. However, its practical implementation still faces significant technical challenges, such as high mobility and limited budget for communication payloads of LEO satellite nodes. This article aims at revealing the main technical issues that have not been fully addressed by the existing LEO-SAN designs, from three aspects namely random access, beam management and Doppler-resistant transmission technologies. More specifically, the critical issues of random access in LEO-SAN are discussed regarding low flexibility, long transmission delay, and inefficient handshakes. Then the beam management for LEO-SAN is investigated in complex propagation environments under the constraints of high mobility and limited payload budget. Furthermore, the influence of Doppler shifts on LEO-SAN is explored. Correspondingly, promising technologies to address these challenges are also discussed, respectively. Finally, the future research directions are envisioned.",
        "bibid":"xiao2022leo"
    },
    {
        "title":"Space-Air-Ground Integrated Multi-Domain Network Resource Orchestration Based on Virtual Network Architecture: A DRL Method",
        "abstract":"Traditional ground wireless communication networks cannot provide high-quality services for artificial intelligence (AI) applications such as intelligent transportation systems (ITS) due to deployment, coverage and capacity issues. The space-air-ground integrated network (SAGIN) has become a research focus in the industry. Compared with traditional wireless communication networks, SAGIN is more flexible and reliable, and it has wider coverage and higher quality of seamless connection. However, due to its inherent heterogeneity, time-varying and self-organizing characteristics, the deployment and use of SAGIN still faces huge challenges, among which the orchestration of heterogeneous resources is a key issue. Based on virtual network architecture and deep reinforcement learning (DRL), we model SAGIN’s heterogeneous resource orchestration as a multi-domain virtual network embedding (VNE) problem, and propose a SAGIN cross-domain VNE algorithm. We model the different network segments of SAGIN, and set the network attributes according to the actual situation of SAGIN and user needs. In DRL, the agent is acted by a five-layer policy network. We build a feature matrix based on network attributes extracted from SAGIN and use it as the agent training environment. Through training, the probability of each underlying node being embedded can be derived. In test phase, we complete the embedding process of virtual nodes and links in turn based on this probability. Finally, we verify the effectiveness of the algorithm from both training and testing.",
        "bibid":"zhang2021space"
    },
    {
        "title":"Joint Multi-Task Offloading and Resource Allocation for Mobile Edge Computing Systems in Satellite IoT",
        "abstract":"For multi-task mobile edge computing (MEC) systems in satellite Internet of Things (IoT), there are dependencies between different tasks, which need to be collected and jointly offloaded. It is crucial to allocate the computing and communication resources reasonably due to the scarcity of satellite communication and computing resources. To address this issue, we propose a joint multi-task offloading and resource allocation scheme in satellite IoT to improve the offloading efficiency. We first construct a novel resource allocation and task scheduling system in which tasks are collected and decided by multiple unmanned aerial vehicles (UAV) based aerial base stations, the edge computing services are provided by satellites. Furthermore, we investigate the multi-task joint computation offloading problem in the framework. Specifically, we model tasks with dependencies as directed acyclic graphs (DAG), then we propose an attention mechanism and proximal policy optimization (A-PPO) collaborative algorithm to learn the best offloading strategy. The simulation results show that the A-PPO algorithm can converge in 25 steps. Furthermore, the A-PPO algorithm reduces cost by at least 8.87 % compared to several baseline algorithms. In summary, this paper provides a new insight for the cost optimization of multi-task MEC systems in satellite IoT.",
        "bibid":"chai2023joint"
    },
    {
        "title":"Double-edge intelligent integrated satellite terrestrial networks",
        "abstract":"The efficient integration of satellite and terrestrial networks has become an important component for 6G wireless architectures to provide highly reliable and secure connectivity over a wide geographical area. As the satellite and cellular networks are developed separately these years, the integrated network should synergize the communication, storage, computation capabilities of both sides towards an intelligent system more than mere consideration of coexistence. This has motivated us to develop double-edge intelligent integrated satellite and terrestrial networks (DILIGENT). Leveraging the boost development of multi-access edge computing (MEC) technology and artificial intelligence (AI), the framework is entitled with the systematic learning and adaptive network management of satellite and cellular networks. In this article, we provide a brief review of the state-of-art contributions from the perspective of academic research and standardization. Then we present the overall design of the proposed DILIGENT architecture, where the advantages are discussed and summarized. Strategies of task offloading, content caching and distribution are presented. Numerical results show that the proposed network architecture outperforms the existing integrated networks.",
        "bibid":"zhang2020double"
    },
    {
        "title":"A Vision of Self-Evolving Network Management for Future Intelligent Vertical HetNet",
        "abstract":"Future integrated terrestrial-aerial-satellite networks will have to exhibit some unprecedented characteristics for the provision of both communications and computation services, and security for a tremendous number of devices with very broad and demanding requirements across multiple networks, operators, and ecosystems. Although 3GPP introduced the concept of self-organizing networks (SONs) in 4G and 5G documents to automate network management, even this progressive concept will face several challenges as it may not be sufficiently agile in coping with the immense levels of complexity, heterogeneity, and mobility in the envisioned beyond-5G integrated networks. In the presented vision, we discuss how future integrated networks can be intelligently and autonomously managed to efficiently utilize resources, reduce operational costs, and achieve the targeted Quality of Experience (QoE). We introduce the novel concept of the “self-evolving networks (SENs)” framework, which utilizes artificial intelligence, enabled by machine learning (ML) algorithms, to make future integrated networks fully automated and intelligently evolve with respect to the provision, adaptation, optimization, and management aspects of networking, communications, computation, and infrastructure nodes' mobility. To envisage the concept of SEN in future integrated networks, we use the Intelligent Vertical Heterogeneous Network (I-VHetNet) architecture as our reference. The article discusses five prominent scenarios where SEN plays the main role in providing automated network management. Numerical results provide an insight into how the SEN framework improves the performance of future integrated networks. The article presents the leading enablers and examines the challenges associated with the application of the SEN concept in future integrated networks.",
        "bibid":"darwish2021vision"
    },
    {
        "title":"Improvement of the Global Connectivity Using Integrated Satellite-Airborne-Terrestrial Networks With Resource Optimization",
        "abstract":"In this paper, we propose a novel wireless scheme that integrates satellite, airborne, and terrestrial networks aiming to support ground users. More specifically, we study the enhancement of the achievable users' throughput assisted with terrestrial base stations, high-altitude platforms (HAPs), and satellite stations. The goal is to optimize the resource allocations and the HAPs' locations in order to maximize the users' throughput. In this context, we formulate and solve an optimization problem in two stages: a short-term stage and a long-term stage. In the short-term stage, we start by proposing an approximated solution and a low complexity solution to solve the associations and power allocations. In the approximated solution, we formulate and solve a binary linear optimization problem to find the best associations and then we use the Taylor expansion approximation to optimally determine the power allocations. In the latter solution, we propose a low complexity approach based on a frequency partitioning technique to solve the associations and power allocations. On the other hand, in the long-term stage, we optimize the locations of the HAPs by proposing an efficient algorithm based on a recursive shrink-and-realign process. Finally, selected numerical results underline the advantages provided by our proposed optimization scheme.",
        "bibid":"alsharoa2020improvement"
    },
    {
        "title":"Artificial Intelligence Techniques for Next-Generation Mega Satellite Networks",
        "abstract":"Space communications, particularly massive satellite networks, re-emerged as an appealing candidate for next generation networks due to major advances in space launching, electronics, processing power, and miniaturization. However, massive satellite networks rely on numerous underlying and intertwined processes that cannot be truly captured using conventionally used models, due to their dynamic and unique features such as orbital speed, inter-satellite links, short pass time, and satellite footprint, among others. Hence, new approaches are needed to enable the network to proactively adjust to the rapidly varying conditions associated within the link. Artificial intelligence (AI) provides a pathway to capture these processes, analyze their behavior, and model their effect on the network. This article introduces the application of AI techniques for integrated terrestrial satellite networks, particularly massive satellite network communications. It details the unique features of massive satellite networks, and the overarching challenges concomitant with their integration into the current communication infrastructure. Moreover, this article provides insights into state-of-the-art AI techniques across various layers of the communication link. This entails applying AI for forecasting the highly dynamic radio channel, spectrum sensing and classification, signal detection and demodulation, inter-satellite and satellite access network optimization, and network security. Moreover, future paradigms and the mapping of these mechanisms onto practical networks are outlined.",
        "bibid":"homssi2022artificial"
    },

    {
        "title":"Dynamically Adaptive Cooperation Transmission among Satellite-Ground Integrated Networks",
        "abstract":"It is a desirable goal to fuse satellite and ground integrated networks (SGINs) to improve the resource utilization efficiency. However, existing work did not consider how to integrate them as a whole network because they lack of function-configurable network management and efficient cooperation among satellite and ground networks. In this paper, we firstly propose a SDN-based network architecture, where resources in SGINs are managed and scheduled in the layered and on-demand way. Then, we formulate the dynamical cooperation transmission in SGINs as an optimization problem and prove its NP hardness. Finally, to realize deeper-level resource fusion, we propose an efficient transmission approach DEEPER (aDaptive satEllitE-ground cooPerativE tRasmission) based on dynamical cooperation among satellite and ground networks, which is network-aware and workload-driven. Comprehensive experiment results demonstrate that our approach outperforms related schemes in terms of network throughput, end-to-end delay, transmission quality and load balancing.",
        "bibid":"tang2020dynamically"
    }
]